{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Let's use an export of [Paul's blog](https://paulbricman.com/reflections) to predict his stance towards 63 statements designed to elicit values about polarizing topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some discussions I've had this past week reminded me of a certain challenge faced by personal knowledge management solutions, in the more general sense of Zettelkasten, digital gardens, and second brains, rather than Obsidian, Roam, and Logseq. I should mention, however, that not many people I know see this as a problem, so it might only be a pet peeve of sorts. The issue I'm referring to is the fact that many PKM solutions make it easy to find ideas you're specifically looking for, but don't make much progress in helping you find ideas you didn't even know you were looking for. They make it easy to index notes in such a way as to find them easily later on, to act as a librarian or cartographer of knowledge, but don't help you as much as I'd like to in finding truly unexpected solutions and connections. The nuance is pretty subtle, so it might not make complete sense yet. The rest of this article explains in more depth what this contrast feels like, why I think open-ended search is a valuable affordance, and how we can bridge the gap.\n",
      "\n",
      "Let's start with linear paper-based notes as a more primitive PKM solution, to make the later contrasts more obvious. If you want to find a specific note based on something you're thinking about, you first have to (1) remember there's a note touching on that, and (2) remember where it is in your notebook. Your main other way of finding ideas is to open the notebook at an arbitrary place, where reading it through from the start is a special case.\n",
      "\n",
      "In building towards modern PKMs, there are two directions we can move in. We can either make the step from analog to digital, or from linear to non-linear. Let's first move to digital and then we'll also go the other route.\n"
     ]
    }
   ],
   "source": [
    "# Get list of all paragraphs used in the blog.\n",
    "import os\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "paragraphs = []\n",
    "for filename in os.listdir('../data/blog/'):\n",
    "    with open('../data/blog/' + filename, 'r') as f:\n",
    "        blog = f.read().split('\\n\\n')[1:]\n",
    "        blog = [e for e in blog if len(sent_tokenize(e)) > 2]\n",
    "        paragraphs.extend(blog)\n",
    "\n",
    "print(*paragraphs[:3], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get polarizing statements and personal valuations (not synced with github)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('~/Downloads/valuations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "emb_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nli_model = CrossEncoder('cross-encoder/nli-deberta-v3-base')\n",
    "lm_model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "lm_tok = AutoTokenizer.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "from src.baselines import infer_embs, infer_nli\n",
    "from src.abduction import infer\n",
    "\n",
    "aggregate_emb = []\n",
    "aggregate_nli_absolute = []\n",
    "aggregate_nli_relative = []\n",
    "aggregate_lm = []\n",
    "\n",
    "for idx, row in df.head(1).iterrows():\n",
    "    emb_probs = []\n",
    "    nli_absolute_probs = []\n",
    "    nli_relative_probs = []\n",
    "    lm_probs = []\n",
    "\n",
    "    for paragraph in paragraphs[:3]:\n",
    "        emb_probs += [infer_embs(paragraph, [row['statement'], row['negation']], encoder=emb_model)[0]]\n",
    "        nli_absolute_probs += [infer_nli(paragraph, [row['statement']], mode='absolute')[0]]\n",
    "        nli_relative_probs += [infer_nli(paragraph, [row['statement'], row['negation']], mode='relative')[0]]\n",
    "        lm_probs += [infer(paragraph, [row['statement'], row['negation']], model=lm_model, tokenizer=lm_tok)[0]]\n",
    "\n",
    "    aggregate_emb += [emb_probs]\n",
    "    aggregate_nli_absolute += [nli_absolute_probs]\n",
    "    aggregate_nli_relative += [nli_relative_probs]\n",
    "    aggregate_lm += [lm_probs]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1a91122b17d1b47166487d00fc45f2a095fc6cbe32258b7f3b0ce045ef7e406"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('velma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
